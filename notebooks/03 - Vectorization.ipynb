{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lookup tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = tf.constant(['beach','mountain','desert'])\n",
    "indices = tf.range(len(cats), dtype=tf.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_init = tf.lookup.KeyValueTensorInitializer(cats, indices) #paired_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_oov_buckets = 1\n",
    "table = tf.lookup.StaticVocabularyTable(table_init, num_oov_buckets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table.lookup(tf.constant(['beach']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.one_hot(table.lookup(cats), cats.shape[0]+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representation learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repr_dim = 2\n",
    "cat_dim = cats.shape[0] + num_oov_buckets\n",
    "repr_init = tf.random.uniform([cat_dim, repr_dim])\n",
    "representation = tf.Variable(repr_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = table.lookup(cats)\n",
    "tf.nn.embedding_lookup(representation, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model with representation learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_inputs = tf.keras.layers.Input(shape=[8], name='numeric_inputs')\n",
    "categories = tf.keras.layers.Input(shape=[], dtype=tf.string, name='categorical_inputs')\n",
    "cat_indices = tf.keras.layers.Lambda(lambda cats: table.lookup(cats), name='categorical_index')(categories)\n",
    "cat_embed = tf.keras.layers.Embedding(input_dim=cat_dim, output_dim=2, name='embedding_layer')(cat_indices)\n",
    "encoded_inputs = tf.keras.layers.concatenate([regular_inputs, cat_embed], name='processed_inputs')\n",
    "outputs = tf.keras.layers.Dense(1)(encoded_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Model(\n",
    "    inputs=[regular_inputs, categories],\n",
    "    outputs = [outputs]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An embedding layer is just a dense layer, without activation function and no biases. However, the embedding layer implemented in keras contains a couple a of performance optimizations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
